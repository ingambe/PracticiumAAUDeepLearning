{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28b98ef",
   "metadata": {},
   "source": [
    "## Pytorch NN API\n",
    "\n",
    "Pytorch already includes everything we need to train a linear model in less than 10 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885a3bc9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c086c7ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worthy-snow-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ingambe/course\" target=\"_blank\">https://wandb.ai/ingambe/course</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ingambe/course/runs/kjh9m1sz\" target=\"_blank\">https://wandb.ai/ingambe/course/runs/kjh9m1sz</a><br/>\n",
       "                Run data is saved locally in <code>/home/ingambe/DeepLearningCourse/2-Linear-networks/Slides/wandb/run-20211115_132525-kjh9m1sz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(kjh9m1sz)</h1><iframe src=\"https://wandb.ai/ingambe/course/runs/kjh9m1sz\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb63f824a60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='course') # specify the project of the current run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630a6827",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4, 5, 6])\n",
    "true_b = 2.4\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee15a747",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 32\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5eb666",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can now iterate over minibtaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5d27e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.7159e-01, -1.0637e-01, -1.2871e+00, -7.2477e-02],\n",
       "         [-3.2422e-01, -8.2392e-01, -8.8265e-01,  9.7382e-01],\n",
       "         [ 4.0551e-01, -1.4486e+00,  1.9984e+00,  7.4032e-01],\n",
       "         [ 3.3081e-01, -1.9210e-01, -2.4347e+00,  1.5945e+00],\n",
       "         [-4.0866e-01,  7.7159e-01, -3.7675e-01,  1.0669e-01],\n",
       "         [ 1.1853e+00, -2.9667e-01,  1.9322e-01,  5.6487e-01],\n",
       "         [-2.4877e-01, -9.2276e-01,  6.9874e-01, -1.8382e-01],\n",
       "         [-9.1143e-01, -2.4409e-01, -6.3650e-01,  1.6281e-01],\n",
       "         [-1.4892e+00, -2.5719e-01, -1.9283e+00, -5.5350e-01],\n",
       "         [ 9.6049e-01,  3.6614e-01, -4.2779e-01,  5.0372e-01],\n",
       "         [ 6.6394e-01,  9.1312e-02,  4.8933e-01,  3.7107e-01],\n",
       "         [-5.7254e-01, -7.1862e-01,  5.2669e-01, -3.6386e-01],\n",
       "         [-1.6643e-01, -1.6302e+00,  3.8755e-01,  6.1777e-01],\n",
       "         [ 9.3432e-01, -3.3258e-01,  8.3840e-01,  3.2955e-03],\n",
       "         [ 6.9672e-01,  3.1405e-01, -1.0044e+00, -3.4184e-01],\n",
       "         [-1.3616e+00,  2.6360e-01, -6.1563e-01, -3.8738e-01],\n",
       "         [-1.3392e+00, -1.5072e+00,  3.7102e+00, -2.6372e-01],\n",
       "         [-1.0857e+00, -2.5679e-01, -2.8105e-01,  1.9232e+00],\n",
       "         [-2.1988e+00, -9.7372e-01,  4.2289e-01, -3.9949e-02],\n",
       "         [ 3.4564e-01,  1.1684e-02,  1.7692e+00, -1.2602e-01],\n",
       "         [ 9.8875e-01,  1.0634e-01,  1.3051e-01,  1.2817e+00],\n",
       "         [ 8.4497e-01,  1.1000e+00,  3.8463e-01, -9.8272e-03],\n",
       "         [-2.7486e-01,  3.5708e-02, -1.2245e+00,  6.4516e-01],\n",
       "         [ 5.7792e-02,  9.0801e-01, -5.6246e-01,  9.1511e-02],\n",
       "         [-5.7697e-01,  1.2225e+00,  9.9951e-01,  1.6941e-02],\n",
       "         [ 6.9949e-01, -1.5314e-01,  3.3665e-01, -8.6915e-01],\n",
       "         [ 1.1923e+00, -1.1237e+00, -7.0181e-01, -1.3543e+00],\n",
       "         [ 9.4238e-01,  5.6429e-01,  1.3636e+00,  2.3187e+00],\n",
       "         [-4.4610e-01, -3.3924e-01,  2.0833e+00, -2.9211e-01],\n",
       "         [ 4.3189e-01,  5.8941e-01,  9.3499e-02, -2.2817e+00],\n",
       "         [ 6.2529e-01,  8.5965e-02,  5.1452e-01,  1.7294e+00],\n",
       "         [-5.6253e-01, -3.7661e-01, -3.8644e-01,  1.8332e+00]]),\n",
       " tensor([[ -5.0728],\n",
       "         [  5.9965],\n",
       "         [ 22.5660],\n",
       "         [  1.1098],\n",
       "         [ -2.2826],\n",
       "         [ 10.1514],\n",
       "         [  7.4524],\n",
       "         [ -0.8114],\n",
       "         [-12.6710],\n",
       "         [  3.9566],\n",
       "         [  8.0840],\n",
       "         [  4.1499],\n",
       "         [ 13.2580],\n",
       "         [  9.6238],\n",
       "         [ -4.3628],\n",
       "         [ -6.6271],\n",
       "         [ 21.8065],\n",
       "         [ 11.2512],\n",
       "         [  3.1951],\n",
       "         [ 11.1335],\n",
       "         [ 12.3500],\n",
       "         [  2.2205],\n",
       "         [ -0.5207],\n",
       "         [ -2.8243],\n",
       "         [  2.1872],\n",
       "         [  0.8056],\n",
       "         [ -3.0083],\n",
       "         [ 23.0894],\n",
       "         [ 11.3253],\n",
       "         [-11.9513],\n",
       "         [ 16.3092],\n",
       "         [ 11.6191]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b571b09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For standard operations, we can **use a framework's predefined layers,**\n",
    "which allow us to focus on the layers used to construct the model\n",
    "rather than having to focus on the implementation.\n",
    "\n",
    "The `Sequential` class defines a container\n",
    "for several layers that will be chained together.\n",
    "Given input data, a `Sequential` instance passes it through\n",
    "the first layer, in turn passing the output\n",
    "as the second layer's input and so forth.\n",
    "\n",
    "The layer is said to be *fully-connected*\n",
    "because each of its inputs is connected to each of its outputs\n",
    "by means of a matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48343d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `nn` is an abbreviation for neural networks\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f23d59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to initialize the model parameters. By default Pytorch initialize the weight using an uniform distribution considering the size of the layer.\n",
    "\n",
    "You should **always** initialize your layer\n",
    "\n",
    "<center><img src=\"images/weights init.jpeg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439d9f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient descent doesn't move you far away from the initial starting point\n",
    "\n",
    "The literature is full of different weight initialization techniques\n",
    "\n",
    "You can write yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30bb9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0198,  0.0025, -0.0127, -0.0071]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01) # net[0] is the first layer\n",
    "net[0].bias.data.fill_(0)\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ca6db",
   "metadata": {},
   "source": [
    "99.9999% of the time you will use one from the literature: [See Pytorch init doc](https://pytorch.org/docs/stable/nn.init.html)\n",
    "\n",
    "I recommend **Xavier normal**. It usually works well.\n",
    "If you have time/ressource, you can try different init and pick the best ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605492ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0697, 0.9464, 0.7670, 0.0187]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "net.apply(_weights_init)\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a47fbb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa0622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we need to define the loss function we will use  \n",
    "The `MSELoss` class computes the mean squared error, also known as squared $L_2$ norm  \n",
    "By default, it returns the average loss over examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2d84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f195d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Weights and Biases can kee an eye on your model, login the structure and the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae50a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fb63f7cc3a0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(net, log=\"all\", criterion=loss, log_freq=1,  log_graph=(True)) #log frequency depend on your training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d40a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last piece of the puzzle, we need to define the optimizer  \n",
    "When we (**instantiate an `SGD` instance,**) we will specify the parameters to optimize over (obtainable from our net via `net.parameters()`), with a dictionary of hyperparameters required by our optimization algorithm  \n",
    "Minibatch stochastic gradient descent just requires that we set the value `lr`, which is set to 0.03 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ec64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4217580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0697, 0.9464, 0.7670, 0.0187]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc550c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's put everything together !\n",
    "\n",
    "The training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
    "\n",
    "For each minibatch, we go through the following ritual:\n",
    "\n",
    "* Generate predictions by calling `net(X)` and calculate the loss `l` (the forward propagation).\n",
    "* Calculate gradients by running the backpropagation.\n",
    "* Update the model parameters by invoking our optimizer.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8520988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.029340332373976707\n",
      "epoch 2, loss 0.00010852736158994958\n",
      "epoch 3, loss 9.307710570283234e-05\n",
      "epoch 4, loss 9.297434735344723e-05\n",
      "epoch 5, loss 9.329492604592815e-05\n",
      "epoch 6, loss 9.327488805865869e-05\n",
      "epoch 7, loss 9.331026376457885e-05\n",
      "epoch 8, loss 9.315063653048128e-05\n",
      "epoch 9, loss 9.358949318993837e-05\n",
      "epoch 10, loss 9.331631736131385e-05\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        optim.zero_grad() # please don't forget!\n",
    "        l.backward() # remember: You need to tell wrt to what the gradient is computed\n",
    "        optim.step() # do a step in the gradient direction\n",
    "    with torch.no_grad():\n",
    "        l = loss(net(features), labels) \n",
    "        print(f'epoch {epoch + 1}, loss {l.item()}')\n",
    "        wandb.log({'loss': l.item()}, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863865c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ⚠️ NEVER FORGET TO ZERO_GRAD THE OPTIMIZER ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5b438",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default the optimizer accumulate the gradient!\n",
    "\n",
    "If you don't set it back to 0, it will keep previous gradient and sum them!\n",
    "\n",
    "If your model doesn't converge check this first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce8575",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's compare the true parameters and the learned one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5925519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([-8.2254e-05,  1.7762e-04, -2.9421e-04,  5.2738e-04])\n",
      "error in estimating b: tensor([-0.0003])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2f62a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can save and load model to reuse them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b444356a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 2.0001, -3.4002,  5.0003,  5.9995]])),\n",
       "             ('0.bias', tensor([2.4003]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f39bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'my_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c04ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This save model's parameters as a dictionnary, but **doesn't save the structure** of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cc584d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 2.0001, -3.4002,  5.0003,  5.9995]])),\n",
       "             ('0.bias', tensor([2.4003]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = nn.Sequential(nn.Linear(4, 1))\n",
    "new_model.load_state_dict(torch.load('my_model.pt'))\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef5cbb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
