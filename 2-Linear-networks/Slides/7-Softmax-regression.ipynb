{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493be603",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# SoftMax regression\n",
    "\n",
    "From regression to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c72039",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3da12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than predicting quantities, we often want to classify things.\n",
    "\n",
    "**Example**: Classify a mail as a spam or not, is there a cat in this image ?, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d497ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**In case of a binary classification**, we only provide one output unit with a Sigmoid applied to it\n",
    "\n",
    "**Why don't we have 2 output units?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381bd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"images/sigmoid.png\" height=\"70%\" width=\"70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80769930",
   "metadata": {},
   "source": [
    "When you want to predict, you apply a threshold (usually 0.5) to know which category was predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bba256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In case of a binary classification with one output unit, we use the **Binary Cross Entropy** loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b26bb12",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ef5ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we have more than one classes, we represent them using encoding\n",
    "\n",
    "This encoding ensure there are no order in the representation\n",
    "if for **{dog, cat, bird, fish}** we were assigning $y \\in \\{1, 2, 3, 4\\}$ we would have assign an **order** and a **value** to each class. We don't want that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ce57c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The usual way to represent categorical data is the *one-hot encoding*.\n",
    "\n",
    "It is a vector with as many components as we have categories.\n",
    "\n",
    "The component corresponding to particular instance's category is set to 1\n",
    "and all other components are set to 0.\n",
    "\n",
    "$$y \\in \\{(1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07b0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To estimate the conditional probabilities of all the possible classes, we need a model with one output per class\n",
    "\n",
    "<center><img src=\"images/softmaxreg.svg\" height=\"70%\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129a38c",
   "metadata": {},
   "source": [
    "Suppose that the entire dataset $\\{\\mathbf{X}, \\mathbf{Y}\\}$ has $n$ examples,\n",
    "where the example indexed by $i$\n",
    "consists of a feature vector $\\mathbf{x}^{(i)}$ and a one-hot label vector $\\mathbf{y}^{(i)}$.\n",
    "We can compare the estimates with reality\n",
    "by checking how probable the actual classes are\n",
    "according to our model, given the features:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "**If $P(\\mathbf{Y} \\mid \\mathbf{X}) = 1$ we have a perfect model!** \n",
    "\n",
    "We want to *maximize* the maximum likelihood. However, in neural network, we want to have a loss we can *minimize*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378f96f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Minimizing the **negative log-likelihood** is equivalent to maximizing the maximum likelihood\n",
    "\n",
    "This loss is called the **cross-entropy loss**.\n",
    "It takes the output layer (called **logit**) and the ground truth, transform them into probabilities and compares with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e9fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "prediction = torch.randn(3, 5) # the per category-logits you've predicted \n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = criterion(prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bbe86",
   "metadata": {},
   "source": [
    "**Tip**: The CrossEntropyLoss allows you to assign weight to each class, it can be usefull if your dataset is **unbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf261dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**For multi class classification problem**: Our model output scalars, we want probabilities.\n",
    "These scalars are called **logits**.\n",
    "\n",
    "To transform a vector of **logits** into a probability vector, we use the **SoftMax** function\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e9090",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need a loss function capable to mesure the quality of our predicted probabilities\n",
    "\n",
    "We rely on the **maximum likelihood** estimation\n",
    "\n",
    "**Softmax** provides a vector $\\hat{\\mathbf{y}}$,\n",
    "which we can interpret as estimated conditional probabilities\n",
    "of each class given any input $\\mathbf{x}$, e.g.,\n",
    "$\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265eb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1) # what is dim 0, what is dim 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf49a83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ⚠️ Cross-Entropy itself apply a SoftMax, if your model outputs probabilities, use the Negative Log Likelood loss ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762a393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f49c2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you want to predict, you simply take the index with the maximum probability as the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2253f8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 2.4403,  1.3832,  1.8918, -0.1694]),\n",
       "indices=tensor([2, 0, 1, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "torch.max(a, dim=1) # torch.max return the maximum value and the corresponding index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba279a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Minimizing the **negative log-likelihood** is equivalent to maximizing the maximum likelihood\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "where for any pair of label $\\mathbf{y}$ and model prediction $\\hat{\\mathbf{y}}$ over $q$ classes,\n",
    "the loss function $l$ is\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "This loss is called the **cross-entropy loss**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
