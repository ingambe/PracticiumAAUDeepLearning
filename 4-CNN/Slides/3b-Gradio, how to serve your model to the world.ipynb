{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f17df5",
   "metadata": {},
   "source": [
    "# Gradio\n",
    "\n",
    "How to serve the model in less than 10 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f33dbc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba45b15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having a great model is only one of the many steps toward a successfull application of deep-learning to a problem  \n",
    "In this notebook we will see how to serve it to the world and allow others to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e9e94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to serve the previously trained MNIST model using *LeCun* CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97080866",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class LeCunRevisited(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeCunRevisited, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv1(x)))\n",
    "        x = self.max_pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "model = LeCunRevisited()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e32f26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We load the weight of the saved neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afad6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mnist.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c3b4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then use **Gradio** to create a web interface in less than 10 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d193cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# we need to give a name to our labels\n",
    "labels = [str(i) for i in range(10)]\n",
    "\n",
    "# define the function that will be called when an image is send to our application\n",
    "def predict(img):\n",
    "    # Gray scale because MNIST are black and white images\n",
    "    img = torchvision.transforms.Grayscale(num_output_channels=1)(torchvision.transforms.ToTensor()(img).unsqueeze(0))\n",
    "    with torch.inference_mode():\n",
    "        prediction = torch.nn.functional.softmax(model(img)[0], dim=0)\n",
    "        confidences = {labels[i]: float(prediction[i]) for i in range(len(labels))}\n",
    "    return confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1a8328",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hint: Set streaming=True for Image component to use live streaming.\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://9e218ec3cba0af34.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9e218ec3cba0af34.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/routes.py\", line 284, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/blocks.py\", line 982, in process_api\n",
      "    result = await self.call_function(fn_index, inputs, iterator)\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/blocks.py\", line 824, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/sv/3vgwx6m9009f3vnn14xwy8_40000gn/T/ipykernel_3948/4012910404.py\", line 9, in predict\n",
      "    img = torchvision.transforms.Grayscale(num_output_channels=1)(torchvision.transforms.ToTensor()(img).unsqueeze(0))\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 135, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 137, in to_tensor\n",
      "    raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\n",
      "TypeError: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/routes.py\", line 284, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/blocks.py\", line 982, in process_api\n",
      "    result = await self.call_function(fn_index, inputs, iterator)\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/gradio/blocks.py\", line 824, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/sv/3vgwx6m9009f3vnn14xwy8_40000gn/T/ipykernel_3948/4012910404.py\", line 9, in predict\n",
      "    img = torchvision.transforms.Grayscale(num_output_channels=1)(torchvision.transforms.ToTensor()(img).unsqueeze(0))\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 135, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 137, in to_tensor\n",
      "    raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\n",
      "TypeError: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "title = \"Handwritten Digits Classifier\"\n",
    "description = \"An handwritten digits classifier using LeCun CNN architecture.\"\n",
    "iface = gr.Interface(predict, \n",
    "                     inputs=gr.Image(shape=(28, 28)), \n",
    "                     outputs=gr.Label(num_top_classes=3), \n",
    "                     live=True, \n",
    "                     title=title,\n",
    "                     description=description,\n",
    "                     interpretation='default').launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a40e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
