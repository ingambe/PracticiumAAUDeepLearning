{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb042ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# LeNet\n",
    "\n",
    "Among the first ground breaking result for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db98b7a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470524da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the 90s, optical character recognition was a hot topic  \n",
    "Especially for banks who had to had to process millions of bank check a year (at that time, credit cards were not as popular as today)\n",
    "\n",
    "In 1998, Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner proposed a CNN architecture and a gradient-based approach to train it on handwritten character recognition (**MNIST** dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdec2d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='images/mnist.png' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf'>LeCun's MNIST paper</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74d68f2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class LeCunRevisited(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeCunRevisited, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2) # original uses AvgPool, kernel size 2 divide the ouput by 4\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv1(x)))\n",
    "        x = self.max_pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x)) # original uses Sigmoid\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e019d6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0525,  0.0065, -0.1625, -0.0853, -0.0129,  0.0669,  0.0608, -0.0669,\n",
       "         -0.1539, -0.0058],\n",
       "        [ 0.0588, -0.0021, -0.1439, -0.1186, -0.0249,  0.0575,  0.0483, -0.0661,\n",
       "         -0.1656,  0.0059]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeCunRevisited()\n",
    "net(torch.randn(2, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82f2dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now solve MNIST using *LeCun* architecture\n",
    "\n",
    "First, let's download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b9dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "mnist_train = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92f67c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n",
      "Image size: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(mnist_train)}')\n",
    "print(f'Number of test examples: {len(mnist_test)}')\n",
    "print(f'Image size: {mnist_train[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1115e7",
   "metadata": {},
   "source": [
    "Notice that the MNIST data from *torchvision* are 28x28 compare to the 32x32 from *LeCun*'s paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f447f5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's always a good idea to look at our data before doing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84024e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show(img, title=None):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show(mnist_train[0][0], mnist_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0f6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's create the dataloaders so we can have batch to train/test our network on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3013da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader_train = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_dataloader_test = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a462f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have to define a *LeCun* model that is compatible with the 28*28 input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3445d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeCunRevisited(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeCunRevisited(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeCunRevisited, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv1(x)))\n",
    "        x = self.max_pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = LeCunRevisited()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bfed2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our traditional training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bbc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f92de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ingambe/LeCunExample/runs/2ccf46w8\" target=\"_blank\">fragrant-plasma-4</a></strong> to <a href=\"https://wandb.ai/ingambe/LeCunExample\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='LeCunExample')\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(mnist_dataloader_train):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.cpu().item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = predicted.cpu()\n",
    "            running_acc += (predicted == labels).float().mean().item()\n",
    "    wandb.log({'loss': running_loss / (i + 1),\n",
    "                'acc': running_acc / (i + 1),\n",
    "                'epoch': epoch + 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75a8f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now need to check the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7292c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images dataset: 98.7%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in mnist_dataloader_test:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the network on the {total} test images dataset: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ffbd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! Now let's save our awesome model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6482ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ingambe/PycharmProjects/PracticumAAUDeepLearning/4-CNN/Slides/wandb/run-20221109_205443-2ccf46w8/files/mnist.pth']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'mnist.pth')\n",
    "wandb.save('mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3becd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
