{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a1c1cb",
   "metadata": {},
   "source": [
    "# FairSeq\n",
    "\n",
    "Using library to help pre-processing and training Seq2Seq models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05026714",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: editdistance in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (0.6.0)\r\n",
      "Requirement already satisfied: torch in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (1.10.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (4.62.3)\r\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (2.0.0)\r\n",
      "Requirement already satisfied: hydra-core in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (1.1.1)\r\n",
      "Requirement already satisfied: cython in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (0.29.26)\r\n",
      "Requirement already satisfied: numpy in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (1.21.4)\r\n",
      "Requirement already satisfied: cffi in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (1.15.0)\r\n",
      "Requirement already satisfied: regex in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (2022.1.18)\r\n",
      "Requirement already satisfied: dataclasses in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from fairseq) (0.8)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\r\n",
      "Requirement already satisfied: colorama in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\r\n",
      "Requirement already satisfied: pycparser in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from cffi->fairseq) (2.21)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from hydra-core->fairseq) (4.8)\r\n",
      "Requirement already satisfied: omegaconf==2.1.* in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from hydra-core->fairseq) (2.1.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from omegaconf==2.1.*->hydra-core->fairseq) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (from torch->fairseq) (4.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f4e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from fairseq import utils\n",
    "from fairseq.models import FairseqEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8032cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "We will use a GRU to encode the input sentence and provide a context to the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c8b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRUEncoder(FairseqEncoder):\n",
    "\n",
    "    def __init__(\n",
    "        self, args, dictionary, embed_dim=128, hidden_dim=128, dropout=0.1,\n",
    "    ):\n",
    "        super().__init__(dictionary)\n",
    "        self.args = args\n",
    "\n",
    "        # Our encoder will embed the inputs before feeding them to the GRU.\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=len(dictionary),\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=dictionary.pad(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll use a single-layer, unidirectional GRU for simplicity.\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # The inputs to the ``forward()`` function are determined by the\n",
    "        # Task, and in particular the ``'net_input'`` key in each\n",
    "        # mini-batch. We discuss Tasks in the next tutorial, but for now just\n",
    "        # know that *src_tokens* has shape `(batch, src_len)` and *src_lengths*\n",
    "        # has shape `(batch)`.\n",
    "\n",
    "        # Note that the source is typically padded on the left. This can be\n",
    "        # configured by adding the `--left-pad-source \"False\"` command-line\n",
    "        # argument, but here we'll make the Encoder handle either kind of\n",
    "        # padding by converting everything to be right-padded.\n",
    "        if self.args.left_pad_source:\n",
    "            # Convert left-padding to right-padding.\n",
    "            src_tokens = utils.convert_padding_direction(\n",
    "                src_tokens,\n",
    "                padding_idx=self.dictionary.pad(),\n",
    "                left_to_right=True\n",
    "            )\n",
    "\n",
    "        # Embed the source.\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "\n",
    "        # Apply dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pack the sequence into a PackedSequence object to feed to the GRU.\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)\n",
    "\n",
    "        # Get the output from the GRU.\n",
    "        _outputs, final_hidden = self.gru(x)\n",
    "\n",
    "        # Return the Encoder's output. This can be any object and will be\n",
    "        # passed directly to the Decoder.\n",
    "        return {\n",
    "            # this will have shape `(bsz, hidden_dim)`\n",
    "            'final_hidden': final_hidden.squeeze(0),\n",
    "        }\n",
    "\n",
    "    # Encoders are required to implement this method so that we can rearrange\n",
    "    # the order of the batch elements during inference (e.g., beam search).\n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        \"\"\"\n",
    "        Reorder encoder output according to `new_order`.\n",
    "\n",
    "        Args:\n",
    "            encoder_out: output from the ``forward()`` method\n",
    "            new_order (LongTensor): desired order\n",
    "\n",
    "        Returns:\n",
    "            `encoder_out` rearranged according to `new_order`\n",
    "        \"\"\"\n",
    "        final_hidden = encoder_out['final_hidden']\n",
    "        return {\n",
    "            'final_hidden': final_hidden.index_select(0, new_order),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cb9b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "We then now build the decoder that takes the input and the context, and generate the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185b0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fairseq.models import FairseqDecoder\n",
    "\n",
    "class SimpleGRUDecoder(FairseqDecoder):\n",
    "\n",
    "    def __init__(\n",
    "        self, dictionary, encoder_hidden_dim=128, embed_dim=128, hidden_dim=128,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__(dictionary)\n",
    "\n",
    "        # Our decoder will embed the inputs before feeding them to the GRU.\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=len(dictionary),\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=dictionary.pad(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            # For the first layer we'll concatenate the Encoder's final hidden\n",
    "            # state with the embedded target tokens.\n",
    "            input_size=encoder_hidden_dim + embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "        )\n",
    "\n",
    "        # Define the output projection.\n",
    "        self.output_projection = nn.Linear(hidden_dim, len(dictionary))\n",
    "\n",
    "    # During training Decoders are expected to take the entire target sequence\n",
    "    # (shifted right by one position) and produce logits over the vocabulary.\n",
    "    # The *prev_output_tokens* tensor begins with the end-of-sentence symbol,\n",
    "    # ``dictionary.eos()``, followed by the target sequence.\n",
    "    def forward(self, prev_output_tokens, encoder_out):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for teacher forcing\n",
    "            encoder_out (Tensor, optional): output from the encoder, used for\n",
    "                encoder-side attention\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the last decoder layer's output of shape\n",
    "                  `(batch, tgt_len, vocab)`\n",
    "                - the last decoder layer's attention weights of shape\n",
    "                  `(batch, tgt_len, src_len)`\n",
    "        \"\"\"\n",
    "        bsz, tgt_len = prev_output_tokens.size()\n",
    "\n",
    "        # Extract the final hidden state from the Encoder.\n",
    "        final_encoder_hidden = encoder_out['final_hidden']\n",
    "\n",
    "        # Embed the target sequence, which has been shifted right by one\n",
    "        # position and now starts with the end-of-sentence symbol.\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "\n",
    "        # Apply dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Concatenate the Encoder's final hidden state to *every* embedded\n",
    "        # target token.\n",
    "        x = torch.cat(\n",
    "            [x, final_encoder_hidden.unsqueeze(1).expand(bsz, tgt_len, -1)],\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # Using PackedSequence objects in the Decoder is harder than in the\n",
    "        # Encoder, since the targets are not sorted in descending length order,\n",
    "        # which is a requirement of ``pack_padded_sequence()``. Instead we'll\n",
    "        # feed nn.GRU directly.\n",
    "        initial_state = (\n",
    "            final_encoder_hidden.unsqueeze(0),  # hidden\n",
    "            torch.zeros_like(final_encoder_hidden).unsqueeze(0),  # cell\n",
    "        )\n",
    "        output, _ = self.gru(\n",
    "            x.transpose(0, 1),  # convert to shape `(tgt_len, bsz, dim)`\n",
    "            initial_state,\n",
    "        )\n",
    "        x = output.transpose(0, 1)  # convert to shape `(bsz, tgt_len, hidden)`\n",
    "\n",
    "        # Project the outputs to the size of the vocabulary.\n",
    "        x = self.output_projection(x)\n",
    "\n",
    "        # Return the logits and ``None`` for the attention weights\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1e1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Register The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe421b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import FairseqEncoderDecoderModel, register_model\n",
    "\n",
    "# Note: the register_model \"decorator\" should immediately precede the\n",
    "# definition of the Model class.\n",
    "\n",
    "@register_model('simple_gru')\n",
    "class SimpleGRUModel(FairseqEncoderDecoderModel):\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        # Models can override this method to add new command-line arguments.\n",
    "        # Here we'll add some new command-line arguments to configure dropout\n",
    "        # and the dimensionality of the embeddings and hidden states.\n",
    "        parser.add_argument(\n",
    "            '--encoder-embed-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the encoder embeddings',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--encoder-hidden-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the encoder hidden state',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--encoder-dropout', type=float, default=0.1,\n",
    "            help='encoder dropout probability',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-embed-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the decoder embeddings',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-hidden-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the decoder hidden state',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-dropout', type=float, default=0.1,\n",
    "            help='decoder dropout probability',\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        # Fairseq initializes models by calling the ``build_model()``\n",
    "        # function. This provides more flexibility, since the returned model\n",
    "        # instance can be of a different type than the one that was called.\n",
    "        # In this case we'll just return a SimpleGRUModel instance.\n",
    "\n",
    "        # Initialize our Encoder and Decoder.\n",
    "        encoder = SimpleGRUEncoder(\n",
    "            args=args,\n",
    "            dictionary=task.source_dictionary,\n",
    "            embed_dim=args.encoder_embed_dim,\n",
    "            hidden_dim=args.encoder_hidden_dim,\n",
    "            dropout=args.encoder_dropout,\n",
    "        )\n",
    "        decoder = SimpleGRUDecoder(\n",
    "            dictionary=task.target_dictionary,\n",
    "            encoder_hidden_dim=args.encoder_hidden_dim,\n",
    "            embed_dim=args.decoder_embed_dim,\n",
    "            hidden_dim=args.decoder_hidden_dim,\n",
    "            dropout=args.decoder_dropout,\n",
    "        )\n",
    "        model = SimpleGRUModel(encoder, decoder)\n",
    "\n",
    "        # Print the model architecture.\n",
    "        print(model)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d88ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Register The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb05ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import register_model_architecture\n",
    "\n",
    "# The first argument to ``register_model_architecture()`` should be the name\n",
    "# of the model we registered above (i.e., 'simple_gru'). The function we\n",
    "# register here should take a single argument *args* and modify it in-place\n",
    "# to match the desired architecture.\n",
    "\n",
    "@register_model_architecture('simple_gru', 'tutorial_simple_gru')\n",
    "def tutorial_simple_gru(args):\n",
    "    # We use ``getattr()`` to prioritize arguments that are explicitly given\n",
    "    # on the command-line, so that the defaults defined below are only used\n",
    "    # when no other value has been specified.\n",
    "    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)\n",
    "    args.encoder_hidden_dim = getattr(args, 'encoder_hidden_dim', 256)\n",
    "    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)\n",
    "    args.decoder_hidden_dim = getattr(args, 'decoder_hidden_dim', 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1064c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download pre-cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7c930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning Moses github repository (for tokenization scripts)...\n",
      "fatal: destination path 'mosesdecoder' already exists and is not an empty directory.\n",
      "Cloning Subword NMT repository (for BPE pre-processing)...\n",
      "fatal: destination path 'subword-nmt' already exists and is not an empty directory.\n",
      "Downloading data from http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz...\n",
      "--2022-01-23 16:10:03--  http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19982877 (19M) [application/x-tar]\n",
      "Saving to: ‘de-en.tgz.1’\n",
      "\n",
      "de-en.tgz.1         100%[===================>]  19,06M  5,31MB/s    in 3,6s    \n",
      "\n",
      "2022-01-23 16:10:07 (5,31 MB/s) - ‘de-en.tgz.1’ saved [19982877/19982877]\n",
      "\n",
      "Data successfully downloaded.\n",
      "x de-en/\n",
      "x de-en/IWSLT14.TED.dev2010.de-en.de.xml\n",
      "x de-en/IWSLT14.TED.dev2010.de-en.en.xml\n",
      "x de-en/IWSLT14.TED.tst2010.de-en.de.xml\n",
      "x de-en/IWSLT14.TED.tst2010.de-en.en.xml\n",
      "x de-en/IWSLT14.TED.tst2011.de-en.de.xml\n",
      "x de-en/IWSLT14.TED.tst2011.de-en.en.xml\n",
      "x de-en/IWSLT14.TED.tst2012.de-en.de.xml\n",
      "x de-en/IWSLT14.TED.tst2012.de-en.en.xml\n",
      "x de-en/IWSLT14.TEDX.dev2012.de-en.de.xml\n",
      "x de-en/IWSLT14.TEDX.dev2012.de-en.en.xml\n",
      "x de-en/README\n",
      "x de-en/train.en\n",
      "x de-en/train.tags.de-en.de\n",
      "x de-en/train.tags.de-en.en\n",
      "pre-processing train data...\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "clean-corpus.perl: processing iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.de & .en to iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean, cutoff 1-175, ratio 1.5\n",
      "..........(100000).......\n",
      "Input sentences: 174443  Output sentences:  167522\n",
      "pre-processing valid/test data...\n",
      "orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de\n",
      "Tokenizer Version 1.1\n",
      "Language: de\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "creating train, valid, test...\n",
      "learn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...\n",
      "100%|####################################| 10000/10000 [00:11<00:00, 866.23it/s]\n",
      "apply_bpe.py to train.de...\n",
      "apply_bpe.py to valid.de...\n",
      "apply_bpe.py to test.de...\n",
      "apply_bpe.py to train.en...\n",
      "apply_bpe.py to valid.en...\n",
      "apply_bpe.py to test.en...\n"
     ]
    }
   ],
   "source": [
    "!bash data/prepare-iwslt14.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b46d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 16:11:03 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='iwslt14.tokenized.de-en/train', validpref='iwslt14.tokenized.de-en/valid', testpref='iwslt14.tokenized.de-en/test', align_suffix=None, destdir='data-bin/iwslt14.tokenized.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8)\n",
      "2022-01-23 16:11:06 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n",
      "2022-01-23 16:11:08 | INFO | fairseq_cli.preprocess | [de] iwslt14.tokenized.de-en/train.de: 160239 sents, 4035591 tokens, 0.0% replaced by <unk>\n",
      "2022-01-23 16:11:08 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n",
      "2022-01-23 16:11:09 | INFO | fairseq_cli.preprocess | [de] iwslt14.tokenized.de-en/valid.de: 7283 sents, 182592 tokens, 0.0192% replaced by <unk>\n",
      "2022-01-23 16:11:09 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n",
      "2022-01-23 16:11:10 | INFO | fairseq_cli.preprocess | [de] iwslt14.tokenized.de-en/test.de: 6750 sents, 161838 tokens, 0.0636% replaced by <unk>\n",
      "2022-01-23 16:11:10 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n",
      "2022-01-23 16:11:12 | INFO | fairseq_cli.preprocess | [en] iwslt14.tokenized.de-en/train.en: 160239 sents, 3949114 tokens, 0.0% replaced by <unk>\n",
      "2022-01-23 16:11:12 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n",
      "2022-01-23 16:11:13 | INFO | fairseq_cli.preprocess | [en] iwslt14.tokenized.de-en/valid.en: 7283 sents, 178622 tokens, 0.00448% replaced by <unk>\n",
      "2022-01-23 16:11:13 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n",
      "2022-01-23 16:11:14 | INFO | fairseq_cli.preprocess | [en] iwslt14.tokenized.de-en/test.en: 6750 sents, 156928 tokens, 0.00892% replaced by <unk>\n",
      "2022-01-23 16:11:14 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iwslt14.tokenized.de-en\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang de --target-lang en \\\n",
    "    --trainpref iwslt14.tokenized.de-en/train --validpref iwslt14.tokenized.de-en/valid --testpref iwslt14.tokenized.de-en/test \\\n",
    "    --destdir data-bin/iwslt14.tokenized.de-en \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a1d76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d30885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\r\n",
      "                     [--log-format LOG_FORMAT]\r\n",
      "                     [--tensorboard-logdir TENSORBOARD_LOGDIR] [--seed SEED]\r\n",
      "                     [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16]\r\n",
      "                     [--fp16] [--memory-efficient-fp16]\r\n",
      "                     [--fp16-no-flatten-grads]\r\n",
      "                     [--fp16-init-scale FP16_INIT_SCALE]\r\n",
      "                     [--fp16-scale-window FP16_SCALE_WINDOW]\r\n",
      "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n",
      "                     [--min-loss-scale MIN_LOSS_SCALE]\r\n",
      "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n",
      "                     [--user-dir USER_DIR]\r\n",
      "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n",
      "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n",
      "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\r\n",
      "                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n",
      "                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\r\n",
      "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n",
      "                     [--profile]\r\n",
      "                     [--criterion {cross_entropy,ctc,adaptive_loss,wav2vec,legacy_masked_lm_loss,nat_loss,label_smoothed_cross_entropy,composite_loss,sentence_prediction,label_smoothed_cross_entropy_with_alignment,masked_lm,sentence_ranking,vocab_parallel_cross_entropy}]\r\n",
      "                     [--tokenizer {nltk,space,moses}]\r\n",
      "                     [--bpe {sentencepiece,fastbpe,gpt2,subword_nmt,hf_byte_bpe,bert,byte_bpe,characters,bytes}]\r\n",
      "                     [--optimizer {nag,adafactor,sgd,adamax,adagrad,adam,lamb,adadelta}]\r\n",
      "                     [--lr-scheduler {fixed,reduce_lr_on_plateau,polynomial_decay,inverse_sqrt,tri_stage,cosine,triangular}]\r\n",
      "                     [--scoring {sacrebleu,bleu,wer,chrf}] [--task TASK]\r\n",
      "                     [--num-workers NUM_WORKERS]\r\n",
      "                     [--skip-invalid-size-inputs-valid-test]\r\n",
      "                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\r\n",
      "                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\r\n",
      "                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\r\n",
      "                     [--dataset-impl DATASET_IMPL]\r\n",
      "                     [--data-buffer-size DATA_BUFFER_SIZE]\r\n",
      "                     [--train-subset TRAIN_SUBSET]\r\n",
      "                     [--valid-subset VALID_SUBSET]\r\n",
      "                     [--validate-interval VALIDATE_INTERVAL]\r\n",
      "                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\r\n",
      "                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\r\n",
      "                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\r\n",
      "                     [--disable-validation]\r\n",
      "                     [--max-tokens-valid MAX_TOKENS_VALID]\r\n",
      "                     [--batch-size-valid BATCH_SIZE_VALID]\r\n",
      "                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\r\n",
      "                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\r\n",
      "                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\r\n",
      "                     [--distributed-rank DISTRIBUTED_RANK]\r\n",
      "                     [--distributed-backend DISTRIBUTED_BACKEND]\r\n",
      "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\r\n",
      "                     [--distributed-port DISTRIBUTED_PORT]\r\n",
      "                     [--device-id DEVICE_ID] [--local-rank LOCAL_RANK]\r\n",
      "                     [--distributed-no-spawn] [--ddp-backend {c10d,no_c10d}]\r\n",
      "                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\r\n",
      "                     [--find-unused-parameters] [--fast-stat-sync]\r\n",
      "                     [--broadcast-buffers]\r\n",
      "                     [--distributed-wrapper {DDP,SlowMo}]\r\n",
      "                     [--slowmo-momentum SLOWMO_MOMENTUM]\r\n",
      "                     [--slowmo-algorithm SLOWMO_ALGORITHM]\r\n",
      "                     [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n",
      "                     [--nprocs-per-node NPROCS_PER_NODE]\r\n",
      "                     [--pipeline-model-parallel]\r\n",
      "                     [--pipeline-balance PIPELINE_BALANCE]\r\n",
      "                     [--pipeline-devices PIPELINE_DEVICES]\r\n",
      "                     [--pipeline-chunks PIPELINE_CHUNKS]\r\n",
      "                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\r\n",
      "                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\r\n",
      "                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\r\n",
      "                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\r\n",
      "                     [--pipeline-checkpoint {always,never,except_last}]\r\n",
      "                     [--zero-sharding {none,os}] [--arch ARCH]\r\n",
      "                     [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]\r\n",
      "                     [--stop-time-hours STOP_TIME_HOURS]\r\n",
      "                     [--clip-norm CLIP_NORM] [--sentence-avg]\r\n",
      "                     [--update-freq UPDATE_FREQ] [--lr LR] [--min-lr MIN_LR]\r\n",
      "                     [--use-bmuf] [--save-dir SAVE_DIR]\r\n",
      "                     [--restore-file RESTORE_FILE]\r\n",
      "                     [--finetune-from-model FINETUNE_FROM_MODEL]\r\n",
      "                     [--reset-dataloader] [--reset-lr-scheduler]\r\n",
      "                     [--reset-meters] [--reset-optimizer]\r\n",
      "                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\r\n",
      "                     [--save-interval SAVE_INTERVAL]\r\n",
      "                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\r\n",
      "                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\r\n",
      "                     [--keep-last-epochs KEEP_LAST_EPOCHS]\r\n",
      "                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\r\n",
      "                     [--no-save] [--no-epoch-checkpoints]\r\n",
      "                     [--no-last-checkpoints] [--no-save-optimizer-state]\r\n",
      "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\r\n",
      "                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\r\n",
      "fairseq-train: error: argument --arch/-a: invalid choice: 'tutorial_simple_gru' (choose from 's2t_berard', 's2t_berard_256_3_3', 's2t_berard_512_3_2', 's2t_berard_512_5_3', 'transformer', 'transformer_iwslt_de_en', 'transformer_wmt_en_de', 'transformer_vaswani_wmt_en_de_big', 'transformer_vaswani_wmt_en_fr_big', 'transformer_wmt_en_de_big', 'transformer_wmt_en_de_big_t2t', 's2t_transformer', 's2t_transformer_s', 's2t_transformer_sp', 's2t_transformer_m', 's2t_transformer_mp', 's2t_transformer_l', 's2t_transformer_lp', 'fconv_self_att', 'fconv_self_att_wp', 'fconv', 'fconv_iwslt_de_en', 'fconv_wmt_en_ro', 'fconv_wmt_en_de', 'fconv_wmt_en_fr', 'fconv_lm', 'fconv_lm_dauphin_wikitext103', 'fconv_lm_dauphin_gbw', 'lstm', 'lstm_wiseman_iwslt_de_en', 'lstm_luong_wmt_en_de', 'lstm_lm', 'multilingual_transformer', 'multilingual_transformer_iwslt_de_en', 'nonautoregressive_transformer', 'nonautoregressive_transformer_wmt_en_de', 'nacrf_transformer', 'iterative_nonautoregressive_transformer', 'iterative_nonautoregressive_transformer_wmt_en_de', 'cmlm_transformer', 'cmlm_transformer_wmt_en_de', 'levenshtein_transformer', 'levenshtein_transformer_wmt_en_de', 'levenshtein_transformer_vaswani_wmt_en_de_big', 'levenshtein_transformer_wmt_en_de_big', 'insertion_transformer', 'bart_large', 'bart_base', 'mbart_large', 'mbart_base', 'mbart_base_wmt20', 'transformer_from_pretrained_xlm', 'transformer_lm', 'transformer_lm_big', 'transformer_lm_baevski_wiki103', 'transformer_lm_wiki103', 'transformer_lm_baevski_gbw', 'transformer_lm_gbw', 'transformer_lm_gpt', 'transformer_lm_gpt2_small', 'transformer_lm_gpt2_medium', 'transformer_lm_gpt2_big', 'wav2vec', 'wav2vec2', 'wav2vec_ctc', 'wav2vec_seq2seq', 'roberta', 'roberta_base', 'roberta_large', 'xlm', 'hf_gpt2', 'hf_gpt2_medium', 'hf_gpt2_large', 'hf_gpt2_xl', 'transformer_align', 'transformer_wmt_en_de_big_align', 'masked_lm', 'bert_base', 'bert_large', 'xlm_base', 'lightconv', 'lightconv_iwslt_de_en', 'lightconv_wmt_en_de', 'lightconv_wmt_en_de_big', 'lightconv_wmt_en_fr_big', 'lightconv_wmt_zh_en_big', 'lightconv_lm', 'lightconv_lm_gbw', 'dummy_model', 'transformer_iwslt_de_en_pipeline_parallel', 'transformer_wmt_en_de_big_pipeline_parallel', 'transformer_lm_megatron', 'transformer_lm_megatron_11b', 'model_parallel_roberta', 'model_parallel_roberta_base', 'model_parallel_roberta_large')\r\n"
     ]
    }
   ],
   "source": [
    "!fairseq-train iwslt14.tokenized.de-en --arch tutorial_simple_gru --optimizer adam --lr 0.005 --max-tokens 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5920b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('deep-learning': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd0dbdcd3b24c586c20b309eff54aa9b697e80778f5a79c7346c0d2776399ed8b7c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
